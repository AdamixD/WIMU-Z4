# MER

## Autorzy
- Marta Sobol
- Maciej KozÅ‚owski
- Adam DÄ…bkowski

## Uruchomienie

```bash
# Klonowanie repozytorium
git clone [<repository_url>](https://github.com/AdamixD/WIMU-Z4.git)
cd WIMU-Z4

# Utworzenie Å›rodowiska wirtualnego
python -m venv venv
source venv/bin/activate  # Linux/macOS
# lub: venv\Scripts\activate  # Windows

# Instalacja zaleÅ¼noÅ›ci
pip install -r requirements.txt
```
Lista dostÄ™pnych komend znajduje siÄ™ w pliku COMMANDS.md

## Analiza literatury

|Publikacja|Link|Komentarz|Kod|Wytrenowane modele|Metryki|Zasoby obliczeniowe|
|-|-|-|-|-|-|-|
|**R. Liyanarachchi, A. Joshi, E. Meijering, 2025** *â€œA Survey on Multimodal Music Emotion Recognition.â€*|[ğŸ”—](https://arxiv.org/abs/2504.18799)|Dziedzina MER przesuwa siÄ™ ku multimodalnoÅ›ci, Å‚Ä…czÄ…c audio, tekst, wideo, sygnaÅ‚y fizjologiczne, dane symboliczne i metadane. Praca porzÄ…dkuje proces MMER w czterech etapach (dobÃ³r danych i modalnoÅ›ci, ekstrakcja cech, fuzja i przetwarzanie, predykcja) oraz omawia fuzjÄ™ wczesnÄ…, poÅ›redniÄ…, pÃ³ÅºnÄ… oraz mechanizmy uwagi i pamiÄ™ci. PrzeglÄ…d dotychczasowej literatury prezentuje przejÅ›cie od klasycznych modeli i SVM do CNN/LSTM, a nastÄ™pnie do transformerÃ³w i metod Å‚Ä…czÄ…cych modalnoÅ›ci. WiÄ™kszoÅ›Ä‡ publicznie dostÄ™pnych zbiorÃ³w ma charakter jednomodalny. Brakuje spÃ³jnych benchmarkÃ³w i ujednoliconych metryk, co ogranicza porÃ³wnywalnoÅ›Ä‡ wynikÃ³w. W kontekÅ›cie wykorzystania krytyczne pozostajÄ… zasoby obliczeniowe i synchronizacja strumieni, natomiast rekomendowane sÄ… lÅ¼ejsze mechanizmy fuzji oraz uczenie transferowe.|âŒ|âŒ|Accuracy <br> Precision <br> Recall <br> F1 <br> AUROC <br> MAE <br> RMSE <br> RÂ² <br> CCC <br> hits@k <br> MAP@k|âŒ|
|**J. Kang, D. Herremans, 2024** *â€œAre We There Yet? A Brief Survey of Music Emotion Prediction Datasets, Models and Outstanding Challengesâ€*|[ğŸ”—](https://arxiv.org/pdf/2406.08809)|PorÃ³wnanie istniejÄ…cych zbiorÃ³w danych i architektur modeli. Najpopularniejszym dostÄ™pnym zbiorem jest **DEAM**, ktÃ³ry zawiera ponad 2000 utworÃ³w muzycznych (gÅ‚Ã³wnie rock i muzyka elektroniczna) w formacie MP3 o dÅ‚ugoÅ›ci 45 sekund sparowanych z wartoÅ›ciami **arousal** i **valence** - dwuwymiarowy model Russell'a reprezentujÄ…cy emocje. Jest on powszechnie wykorzystywany w MER do adnotowania. Coraz czÄ™Å›ciej moÅ¼na spotkaÄ‡ siÄ™ z podejÅ›ciami wielomodalnymi w celu poprawy jakoÅ›ci predykcji - zwiÄ…zane jest to z rÃ³Å¼nymi ÅºrÃ³dÅ‚ami bodÅºcÃ³w sensorycznych u czÅ‚owieka, np. modele wykorzystujÄ…ce poza samym dÅºwiÄ™kiem takÅ¼e wideo.|âŒ|âŒ|Accuracy <br> F1 <br> PR-AUC <br> ROC-AUC <br> RMSE <br> RÂ² <br> CCC <br> Pearson correlation |âŒ|
|**Pedro Lima Louro, Hugo Redinho, Ricardo Malheiro, Rui Pedro Paiva, Renato Panda, 2024** *â€œA Comparison Study of Deep Learning Methodologies for Music Emotion Recognition.â€*|[ğŸ”—](https://www.mdpi.com/1424-8220/24/7/2201)|ArtykuÅ‚ porÃ³wnuje klasyczne metody uczenia maszynowego i metody uczenia gÅ‚Ä™bokiego w zadaniu klasyfikacji emocji 4Q. Autorzy przeprowadzili eksperymenty z rÃ³Å¼nymi architekturami modeli, technikami augmetacji danych, sposobami reprezentacji danych oraz uczeniem transferowym. Najlepsze wyniki uzyskano przy zastosowaniu podejÅ›cia hybrydowego, Å‚Ä…czacego CNN trenowanego na rozszerzonym zbiorze danych i DNN wykorzystujÄ…cego mel-spektrogramy oraz rÄ™cznie wyekstrahowane cechy. Ten model osiÄ…gnÄ…Å‚ 80,2% F1-score, co stanowiÅ‚o znacznÄ… poprawÄ™ w porÃ³wnaniu do najlepszych modeli bazowych. Ponadto pokazano, Å¼e zwiÄ™kszenie iloÅ›ci danych miaÅ‚o wiÄ™kszy wpÅ‚yw niÅ¼ rÃ³wnowaÅ¼enie klas, a klasyczne techniki augmentacji poprawiaÅ‚y skutecznoÅ›Ä‡ modeli. Natomiast zastosowanie architektur dziaÅ‚ajÄ…cych na poziomie segmentÃ³w (segment-level), uczenia transferowego lub embeddingÃ³w, nie przyniosÅ‚o poprawy wynikÃ³w - byÅ‚y one gorsze od modeli bazowych.|âŒ|âŒ|Precision <br> Recall <br> F1|Eksperymenty byÅ‚y przeprowadzane na wspÃ³Å‚dzielonym serwerze z dwoma procesorami Intel Xeon Silver 4214 (48 rdzeni, 2,2 GHz) oraz trzema kartami NVIDIA Quadro P500 (16 GB), a w razie potrzeby korzystano takÅ¼e z Google Colab z kartami NVIDIA P100 lub T4.|
|**Pedro Lima Louro, Hugo Redinho, Ricardo Santos, Ricardo Malheiro, Renato Panda, Rui Pedro Paiva, 2025** *â€œMERGE â€” A Bimodal Audio-Lyrics Dataset for Static Music Emotion Recognitionâ€*|[ğŸ”—](https://arxiv.org/abs/2407.06060)|ArtykuÅ‚ stanowi odpowiedÅº na brak publicznych, duÅ¼ych i kontrolowanych jakoÅ›ciowo zbiorÃ³w bimodalnych audio+tekst dla MER. Autorzy przedstawiajÄ… trzy nowe zbiory: MERGE Audio, MERGE Lyrics oraz MERGE Bimodal, etykietowane w czterech Ä‡wiartkach Russella (valenceâ€“arousal). Dane powstaÅ‚y pÃ³Å‚automatycznie na bazie metadanych i klipÃ³w z bazy AllMusic, z kontrolÄ… jakoÅ›ci i standaryzacjÄ… prÃ³bek. |âŒ|âŒ|F1 <br> RMSE <br> RÂ²|âŒ|
|**Essentia**|[ğŸ”—](https://essentia.upf.edu/models.html)|Serwis udostÄ™pnia pre-trenowane modele do analizy muzyki wraz z wagami, metadanymi i przykÅ‚adami uÅ¼ycia.|âœ”ï¸|âœ”ï¸|Metryki sÄ… zrÃ³Å¼nicowane w zaleÅ¼noÅ›ci od rozpatrywanego modelu|âŒ|

## Zbiory danych

**DEAM**

| Parametr             | WartoÅ›Ä‡                  |
|----------------------|--------------------------|
| Liczba utworÃ³w       | 1802                     |
| Typ adnotacji        | Dynamiczne (per sekunda) |
| Reprezentacja emocji | VA                       |

**PMEmo**

| Parametr             | WartoÅ›Ä‡                  |
|----------------------|--------------------------|
| Liczba utworÃ³w       | 767                      |
| Typ adnotacji        | Dynamiczne (per sekunda) |
| Reprezentacja emocji | VA                       |

**MERGE**

| Parametr              | WartoÅ›Ä‡                |
|-----------------------|------------------------|
| Liczba utworÃ³w        | 3554                   |
| Typ adnotacji         | Statyczne (caÅ‚y utwÃ³r) |
| Predefiniowane splity | 70/15/15 lub 40/30/30  |
| Reprezentacja emocji  | VA lub Russell4Q       |


## Eksperymenty

### Metryki ewaluacji
W eksperymentach wykorzystano nastÄ™pujÄ…ce metryki:
- **CCC (Concordance Correlation Coefficient)** - dla trybu VA, mierzy zgodnoÅ›Ä‡ miÄ™dzy predykcjami a wartoÅ›ciami rzeczywistymi, uwzglÄ™dniajÄ…c zarÃ³wno korelacjÄ™ jak i Å›rednie wartoÅ›ci
- **F1 Score (weighted)** - dla trybu Russell4Q, harmoniczna Å›rednia precyzji i recall, waÅ¼ona rozmiarem klas
  
### Metodologia eksperymentÃ³w
KaÅ¼dy eksperyment skÅ‚adaÅ‚ siÄ™ z dwÃ³ch faz:

**Faza 1: Optymalizacja hiperparametrÃ³w**
- 10 triali Optuna z algorytmem TPE
- Walidacja k-fold (k=5) dla DEAM i PMEmo
- Predefiniowane splity train/valid/test (70/15/15) dla MERGE
- Metryka optymalizacji: CCC_mean (VA) lub F1 (Russell4Q)

**Faza 2: Trening finalnego modelu**
- Wykorzystanie najlepszych znalezionych hiperparametrÃ³w
- Trening na peÅ‚nym zbiorze treningowym
- Ewaluacja na zbiorze testowym

### Wyniki eksperymentÃ³w
Otrzymane wyniki eksperymentÃ³w na zbiorze testowym dla najlepszego modelu

**Tryb VA**

| ZbiÃ³r danych | GÅ‚owa BiGRU   | GÅ‚owa CNNLSTM |
|--------------|---------------|---------------|
| DEAM         | 0.637         | 0.725         |
| PMEmo        | 0.646         | 0.710         |
| Merge        | 0.470         | 0.427         |

**Tryb Russell4Q**

W tym trybie etykiety VA dla zbiorÃ³w DEAM i PMEmo sÄ… mapowane do kwadrantÃ³w modelu Russella.

| ZbiÃ³r danych | GÅ‚owa BiGRU | GÅ‚owa CNNLSTM |
|--------------|-------------|---------------|
| DEAM         | 0.623       | 0.698         |
| PMEmo        | 0.670       | 0.734         |
| Merge        | 0.548       | 0.529         |

### Augmentacje
- shift â€“ przesuniÄ™cie czasowe sygnaÅ‚u.
- gain â€“ zmiana gÅ‚oÅ›noÅ›ci nagrania.
- reverb â€“ dodanie pogÅ‚osu do sygnaÅ‚u.
- lowpass â€“ zastosowanie filtru dolnoprzepustowego.
- highpass â€“ zastosowanie filtru gÃ³rnoprzepustowego.
- bandpass â€“ filtr pasmowy przepuszczajÄ…cy wybrane czÄ™stotliwoÅ›ci.
- pitch_shift â€“ zmiana wysokoÅ›ci tonu nagrania.

### Wyniki
Otrzymane wyniki na zbiorze testowym uzyskano przy treningu, w ktÃ³rym dla kaÅ¼dej augmentacji 30% oryginalnych danych byÅ‚o przetwarzanych w formie augmentowanej i dodawanych do zbioru treningowego.

**PMEmo**
| Tryb / Model | BiGRU | CNNLSTM |
|--------------|-------|----------|
| VA           | 0.7160 | 0.7638 |
| Russell4Q    | 0.7434 | 0.8012 |

**Merge**
| Tryb / Model | BiGRU | CNNLSTM |
|--------------|-------|----------|
| VA           | 0.4879 | 0.4779 |
| Russell4Q    | 0.5614 | 0.5399 |


### Wnioski
**PorÃ³wnanie gÅ‚Ã³w**

GÅ‚owa CNNLSTM osiÄ…ga zauwaÅ¼alnie lepsze wyniki niÅ¼ BiGRU (przewaga 10-14%) na zbiorach DEAM i PMEmo w obu trybach, natomiast dla zbioru Merge lepsze rezultaty uzyskuje BiGRU. Wskazuje to, Å¼e w przypadku danych dynamicznych skuteczniejsza jest architektura CNNLSTM, ktÃ³ra umoÅ¼liwia lepsze modelowanie zaleÅ¼noÅ›ci czasowych. Z kolei dla danych statycznych korzystniejsza okazuje siÄ™ prostsza architektura BiGRU, charakteryzujÄ…ca siÄ™ lepszÄ… zdolnoÅ›ciÄ… do generalizacji.

**PorÃ³wnanie zbiorÃ³w danych**

Dla zbioru Merge uzyskane wyniki sÄ… wyraÅºnie niÅ¼sze, niezaleÅ¼nie od zastosowanego trybu, co wskazuje, Å¼e jest on najbardziej wymagajÄ…cym z analizowanych zbiorÃ³w danych. Sugeruje to, Å¼e statyczne adnotacje emocji stanowiÄ… wiÄ™ksze wyzwanie dla zastosowanych modeli, ktÃ³re znacznie lepiej radzÄ… sobie z adnotacjami dynamicznymi. Prawdopodobnie wynika to z faktu, Å¼e statyczne etykiety, przypisane do caÅ‚ego utworu, nie pozwalajÄ… w peÅ‚ni wykorzystaÄ‡ potencjaÅ‚u architektur sekwencyjnych, zaprojektowanych do modelowania zaleÅ¼noÅ›ci czasowych.

NajwyÅ¼sze wyniki uzyskano dla zbioru PMEmo, jednak rÃ³Å¼nice w porÃ³wnaniu do zbioru DEAM sÄ… stosunkowo niewielkie. MoÅ¼e to wskazywaÄ‡, Å¼e oba zbiory charakteryzujÄ… siÄ™ podobnym poziomem trudnoÅ›ci oraz spÃ³jnoÅ›ciÄ… adnotacji, a zastosowane modele efektywnie wykorzystujÄ… dynamicznÄ… reprezentacjÄ™ emocji w obu przypadkach.

**PorÃ³wnanie trybÃ³w**

NajwiÄ™ksze rÃ³Å¼nice miÄ™dzy trybami VA i Russell4Q widoczne sÄ… dla zbioru Merge, gdzie lepsze wyniki uzyskano w trybie Russell4Q. Dla PMEmo Russell4Q rÃ³wnieÅ¼ jest nieznacznie lepszy. Jedynie w zbiorze DEAM tryb VA daje nieco lepsze rezultaty.

W zbiorach DEAM i PMEmo wartoÅ›ci VA zostaÅ‚y mapowane na kwadranty Russella, mimo to dyskretna reprezentacja zachowuje istotne informacje i pozwala modelom skutecznie uczyÄ‡ siÄ™ wzorcÃ³w emocjonalnych.

**Augmentacje**

Dodanie augmentacji poprawia wyniki modeli, co jest szczegÃ³lnie widoczne w przypadku zbioru PMEmo (poprawa o 7â€“11%). MoÅ¼e to wynikaÄ‡ z faktu, Å¼e jest to najmniejszy ze zbiorÃ³w (tylko 767 utworÃ³w), a wprowadzenie danych augmentowanych pozwoliÅ‚o zwiÄ™kszyÄ‡ liczbÄ™ prÃ³bek treningowych. Dla zbioru Merge poprawa wynikÃ³w jest natomiast jedynie nieznaczna, co prawdopodobnie wynika z jego duÅ¼ej wielkoÅ›ci (3554 utworÃ³w). Wynika z tego, Å¼e stosowanie augmentacji jest szczegÃ³lnie korzystne dla mniejszych zbiorÃ³w danych.

### Aplikacja webowa

**FunkcjonalnoÅ›ci**

1. **Åadowanie modeli** - wybÃ³r z dostÄ™pnych modeli .pth
2. **Upload audio** - wgrywanie plikÃ³w MP3/WAV
3. **Wizualizacja VA** - wykres valence/arousal w czasie
4. **Wizualizacja Russell4Q** - rozkÅ‚ad kwadrantÃ³w
5. **PorÃ³wnanie modeli** - analiza dwÃ³ch modeli jednoczeÅ›nie
6. **Odtwarzacz audio** - synchronizacja z wizualizacjami

**Interfejs**



