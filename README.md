# MER

## Autorzy
- Marta Sobol
- Maciej KozÅ‚owski
- Adam DÄ…bkowski

## Analiza literatury

|Publikacja|Link|Komentarz|Kod|Wytrenowane modele|Metryki|Zasoby obliczeniowe|
|-|-|-|-|-|-|-|
|**R. Liyanarachchi, A. Joshi, E. Meijering, 2025** *â€œA Survey on Multimodal Music Emotion Recognition.â€*|[ğŸ”—](https://arxiv.org/abs/2504.18799)|Dziedzina MER przesuwa siÄ™ ku multimodalnoÅ›ci, Å‚Ä…czÄ…c audio, tekst, wideo, sygnaÅ‚y fizjologiczne, dane symboliczne i metadane. Praca porzÄ…dkuje proces MMER w czterech etapach (dobÃ³r danych i modalnoÅ›ci, ekstrakcja cech, fuzja i przetwarzanie, predykcja) oraz omawia fuzjÄ™ wczesnÄ…, poÅ›redniÄ…, pÃ³ÅºnÄ… oraz mechanizmy uwagi i pamiÄ™ci. PrzeglÄ…d dotychczasowej literatury prezentuje przejÅ›cie od klasycznych modeli i SVM do CNN/LSTM, a nastÄ™pnie do transformerÃ³w i metod Å‚Ä…czÄ…cych modalnoÅ›ci. WiÄ™kszoÅ›Ä‡ publicznie dostÄ™pnych zbiorÃ³w ma charakter jednomodalny. Brakuje spÃ³jnych benchmarkÃ³w i ujednoliconych metryk, co ogranicza porÃ³wnywalnoÅ›Ä‡ wynikÃ³w. W kontekÅ›cie wykorzystania krytyczne pozostajÄ… zasoby obliczeniowe i synchronizacja strumieni, natomiast rekomendowane sÄ… lÅ¼ejsze mechanizmy fuzji oraz uczenie transferowe.|âŒ|âŒ|Accuracy <br> Precision <br> Recall <br> F1 <br> AUROC <br> MAE <br> RMSE <br> RÂ² <br> CCC <br> hits@k <br> MAP@k|âŒ|
|**J. Kang, D. Herremans, 2024** *â€œAre We There Yet? A Brief Survey of Music Emotion Prediction Datasets, Models and Outstanding Challengesâ€*|[ğŸ”—](https://arxiv.org/pdf/2406.08809)|PorÃ³wnanie istniejÄ…cych zbiorÃ³w danych i architektur modeli. Najpopularniejszym dostÄ™pnym zbiorem jest **DEAM**, ktÃ³ry zawiera ponad 2000 utworÃ³w muzycznych (gÅ‚Ã³wnie rock i muzyka elektroniczna) w formacie MP3 o dÅ‚ugoÅ›ci 45 sekund sparowanych z wartoÅ›ciami **arousal** i **valence** - dwuwymiarowy model Russell'a reprezentujÄ…cy emocje. Jest on powszechnie wykorzystywany w MER do adnotowania. Coraz czÄ™Å›ciej moÅ¼na spotkaÄ‡ siÄ™ z podejÅ›ciami wielomodalnymi w celu poprawy jakoÅ›ci predykcji - zwiÄ…zane jest to z rÃ³Å¼nymi ÅºrÃ³dÅ‚ami bodÅºcÃ³w sensorycznych u czÅ‚owieka, np. modele wykorzystujÄ…ce poza samym dÅºwiÄ™kiem takÅ¼e wideo.|âŒ|âŒ|Accuracy <br> F1 <br> PR-AUC <br> ROC-AUC <br> RMSE <br> RÂ² <br> CCC <br> Pearson correlation |âŒ|
|**Pedro Lima Louro, Hugo Redinho, Ricardo Malheiro, Rui Pedro Paiva, Renato Panda, 2024** *â€œA Comparison Study of Deep Learning Methodologies for Music Emotion Recognition.â€*|[ğŸ”—](https://www.mdpi.com/1424-8220/24/7/2201)|ArtykuÅ‚ porÃ³wnuje klasyczne metody uczenia maszynowego i metody uczenia gÅ‚Ä™bokiego w zadaniu klasyfikacji emocji 4Q. Autorzy przeprowadzili eksperymenty z rÃ³Å¼nymi architekturami modeli, technikami augmetacji danych, sposobami reprezentacji danych oraz uczeniem transferowym. Najlepsze wyniki uzyskano przy zastosowaniu podejÅ›cia hybrydowego, Å‚Ä…czacego CNN trenowanego na rozszerzonym zbiorze danych i DNN wykorzystujÄ…cego mel-spektrogramy oraz rÄ™cznie wyekstrahowane cechy. Ten model osiÄ…gnÄ…Å‚ 80,2% F1-score, co stanowiÅ‚o znacznÄ… poprawÄ™ w porÃ³wnaniu do najlepszych modeli bazowych. Ponadto pokazano, Å¼e zwiÄ™kszenie iloÅ›ci danych miaÅ‚o wiÄ™kszy wpÅ‚yw niÅ¼ rÃ³wnowaÅ¼enie klas, a klasyczne techniki augmentacji poprawiaÅ‚y skutecznoÅ›Ä‡ modeli. Natomiast zastosowanie architektur dziaÅ‚ajÄ…cych na poziomie segmentÃ³w (segment-level), uczenia transferowego lub embeddingÃ³w, nie przyniosÅ‚o poprawy wynikÃ³w - byÅ‚y one gorsze od modeli bazowych.|âŒ|âŒ|Precision <br> Recall <br> F1|Eksperymenty byÅ‚y przeprowadzane na wspÃ³Å‚dzielonym serwerze z dwoma procesorami Intel Xeon Silver 4214 (48 rdzeni, 2,2 GHz) oraz trzema kartami NVIDIA Quadro P500 (16 GB), a w razie potrzeby korzystano takÅ¼e z Google Colab z kartami NVIDIA P100 lub T4.|
|**Pedro Lima Louro, Hugo Redinho, Ricardo Santos, Ricardo Malheiro, Renato Panda, Rui Pedro Paiva, 2025** *â€œMERGE â€” A Bimodal Audio-Lyrics Dataset for Static Music Emotion Recognitionâ€*|[ğŸ”—](https://arxiv.org/abs/2407.06060)|ArtykuÅ‚ stanowi odpowiedÅº na brak publicznych, duÅ¼ych i kontrolowanych jakoÅ›ciowo zbiorÃ³w bimodalnych audio+tekst dla MER. Autorzy przedstawiajÄ… trzy nowe zbiory: MERGE Audio, MERGE Lyrics oraz MERGE Bimodal, etykietowane w czterech Ä‡wiartkach Russella (valenceâ€“arousal). Dane powstaÅ‚y pÃ³Å‚automatycznie na bazie metadanych i klipÃ³w z bazy AllMusic, z kontrolÄ… jakoÅ›ci i standaryzacjÄ… prÃ³bek. |âŒ|âŒ|F1 <br> RMSE <br> RÂ²|âŒ|
|**Essentia**|[ğŸ”—](https://essentia.upf.edu/models.html)|Serwis udostÄ™pnia pre-trenowane modele do analizy muzyki wraz z wagami, metadanymi i przykÅ‚adami uÅ¼ycia.|âœ”ï¸|âœ”ï¸|Metryki sÄ… zrÃ³Å¼nicowane w zaleÅ¼noÅ›ci od rozpatrywanego modelu|âŒ|


## Status realizacji

âœ”ï¸ Wykonano

- Analiza wymagaÅ„ i literatury z zakresu MER.
- Analiza wybranych zbiorÃ³w danych (DEAM, emoMusic, MERGE) i przygotowanie Å›rodowiska.
- Implementacja prototypu bazowego na danych pozbawionych dodatkowego przeprocesowania (czyszczenia i augmentacji). Prototyp umoÅ¼liwia wczytanie pliku audio, jego analizÄ™ i zwrÃ³cenie predykcji (tryb VA).
- PorÃ³wnanie wynikÃ³w otrzymanego prototypu z modelami dostÄ™pnymi w Essentia (tryb VA).

ğŸš§ W trakcie realizacji

- Dostosowanie struktury repozytorium do szablonu *cookiecutter-data-science*
- Integracja z tensorboard
- Eksperymenty z rÃ³Å¼nymi architekturami modeli oraz analiza wpÅ‚ywu augmentacji danych
- Opracowanie aplikacji webowej

## Do pobrania
### DEAM dataset
audio - https://cvml.unige.ch/databases/DEAM/DEAM_audio.zip do katalogu `/data/DEAM/audio/`

annotations - https://cvml.unige.ch/databases/DEAM/DEAM_Annotations.zip do katalogu `/data/DEAM/annotations/`
